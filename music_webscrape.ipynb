{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "music_webscrape.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BStricks/music_information_retrieval/blob/master/music_webscrape.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxsbXsrFojHs",
        "colab_type": "text"
      },
      "source": [
        "# Web scraping the album review corpus\n",
        "\n",
        "The primary purpose of this script is to crawl the pitchfork.com website for all album reviews and download the review text into a dataframe along with the artist and album name attributes. This methodology can be extended to include multiple other domains e.g. amazon reviews, rolling stone etc.\n",
        "\n",
        "The secondary purpose was to trial a document matching algorithm on the newly created corpus; using a range of matching techniques the aim is to match a user's natuaral language query with the most appropriate album. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrMFbZgGqZy5",
        "colab_type": "text"
      },
      "source": [
        "# Section 1: web scraping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxCUMWqSUJcM",
        "colab_type": "code",
        "outputId": "2b16e88a-b414-4b59-894b-97a12d5b0837",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "###mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "###change directory\n",
        "%cd gdrive/My Drive/Colab Notebooks/album_reviews"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "[Errno 2] No such file or directory: 'gdrive/My Drive/Colab Notebooks/album_reviews'\n",
            "/content/gdrive/My Drive/Colab Notebooks/album_reviews\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGqpZUym6uHJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###libraries\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "import pickle\n",
        "!wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hbf5fJN5mOGI",
        "colab_type": "text"
      },
      "source": [
        "## Pitchfork scrape"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9iLsRmI7dQS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###webpages to scrape\n",
        "pagelist = []\n",
        "for i in range(1, 50):\n",
        "  pagelist.append('https://pitchfork.com/reviews/albums/?page='+str(i))\n",
        "\n",
        "###create table for hyperlinks\n",
        "master_table_pitchfork = pd.DataFrame(columns=['href', 'artist', 'album'])\n",
        "\n",
        "###function to scrape hyperlinks and extract artist/album tags\n",
        "for i in pagelist:\n",
        "\n",
        "  page = requests.get(i)\n",
        "  soup = BeautifulSoup(page.text, 'html.parser').find_all('div', attrs={\"class\":\"review\"})\n",
        "\n",
        "  for div in soup:\n",
        "    href = ['https://pitchfork.com/'+div.find('a',attrs={\"class\":\"review__link\"})['href']]\n",
        "    artist = [div.find('li').text]\n",
        "    album = [div.find('h2').text]\n",
        "\n",
        "    new_table = pd.DataFrame(\n",
        "        {'href': href,\n",
        "        'artist': artist,\n",
        "        'album': album\n",
        "        })\n",
        "\n",
        "    master_table_pitchfork = master_table_pitchfork.append(new_table)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CA3MaBPWMfkz",
        "colab": {}
      },
      "source": [
        "###scrape webpage for album review text\n",
        "review_text = []\n",
        "\n",
        "for i in range(0,588):\n",
        "  \n",
        "  href = master_table_pitchfork.iloc[i][0]\n",
        "  page = requests.get(href)\n",
        "\n",
        "  if not page:\n",
        "    review_text.append(\"NULL\")\n",
        "\n",
        "  else: \n",
        "    soup = BeautifulSoup(page.text, 'html.parser').find_all('div', attrs={\"class\":\"contents\"})\n",
        "  \n",
        "    for div in soup:\n",
        "    \n",
        "      if div.text:\n",
        "        review_text.append(div.text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqOCbFFGHMrg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "master_table_pitchfork = master_table_pitchfork.assign(review_text=review_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9294pxEDmgvL",
        "colab_type": "text"
      },
      "source": [
        "## NME scrape"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clpebcswmjsx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###webpages to scrape\n",
        "pagelist = []\n",
        "for i in range(1, 2):\n",
        "  pagelist.append('https://www.nme.com/reviews/album/page/'+str(i))\n",
        "\n",
        "###create table for hyperlinks\n",
        "master_table_nme = pd.DataFrame(columns=['href', 'artist','album'])\n",
        "\n",
        "###function to scrape hyperlinks and extract artist/album tags\n",
        "for i in pagelist:\n",
        "\n",
        "  page = requests.get(i)\n",
        "  soup = BeautifulSoup(page.text, 'html.parser').find_all('li', attrs={\"class\":\"listing-item\"})\n",
        "  href = []\n",
        "  artist = []\n",
        "  album = []\n",
        "\n",
        "  for i in soup:\n",
        "    \n",
        "    for a in i.find_all('a'):\n",
        "      href.append(a['href'])\n",
        "\n",
        "    for header in i.find_all(\"h3\"):\n",
        "      header_1 = header.text.strip()\n",
        "      artist1 = header_1.split(' –')[0]\n",
        "      artist.append(artist1)\n",
        "      try: \n",
        "        album1 = header_1.split('\\'')[1]\n",
        "        album2 = album1.split('\\'')[0]\n",
        "        album.append(album2)\n",
        "      except:\n",
        "        album1 = header_1.split('‘')[1]\n",
        "        album2 = album1.split('’')[0]\n",
        "        album.append(album2)\n",
        "\n",
        "new_table = pd.DataFrame({'href': href,'artist': artist,'album': album})\n",
        "\n",
        "master_table_nme = master_table_nme.append(new_table)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWvyeY3LyNhe",
        "colab_type": "code",
        "outputId": "ff849c85-20db-495c-88a6-490aed02d829",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(len(master_table_nme))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "31\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOhye95qmqh0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###scrape webpage for album review text\n",
        "review_text = []\n",
        "\n",
        "for i in range(0,31):\n",
        "  \n",
        "  href = master_table_nme.iloc[i][0]\n",
        "  page = requests.get(href)\n",
        "\n",
        "  if not page:\n",
        "    review_text.append(\"NULL\")\n",
        "\n",
        "  else: \n",
        "    soup = BeautifulSoup(page.text, 'html.parser').find_all('p')   \n",
        "    sentences = []\n",
        "    for p in soup:\n",
        "        if p.text:\n",
        "          para = str(p.text.strip())\n",
        "          if para.startswith(\"window\"):\n",
        "            pass\n",
        "          elif para.startswith(\"Release\"):\n",
        "            pass\n",
        "          elif para.startswith(\"Record\"):\n",
        "            pass\n",
        "          else:\n",
        "            sentences.append(para)\n",
        "  \n",
        "  review_text.append(' '.join(sentences))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qxB1QK0d99I",
        "colab_type": "code",
        "outputId": "1d2fa3f7-d7f1-46e0-9b7c-36edea08e816",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(len(review_text))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "31\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pcw_vEPxrCsW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "master_table_nme = master_table_nme.assign(review_text=review_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbqBACOOrQoK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#combine\n",
        "master_table = master_table_pitchfork.append(master_table_nme)\n",
        "\n",
        "#pickle\n",
        "outfile = open('album_corpus','wb')\n",
        "pickle.dump(master_table,outfile)\n",
        "outfile.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vs6hDVBIqilw",
        "colab_type": "text"
      },
      "source": [
        "# Section 2: Sentence matching"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ollDq570q4G-",
        "colab_type": "text"
      },
      "source": [
        "## Approach 1 - sentence level matching\n",
        "The first approach taken is to split each review into sentence level data and then perform matching on a 'user -> review sentence' basis. It is hypothesised that given the complex nature of reviews that this will perform better than creating a document level match.\n",
        "\n",
        "Features:\n",
        "*   Vector space model - count vectors (1-4gram)\n",
        "*   Vector space model - tfidf vectors (1-4gram)\n",
        "*   Topic model - LSA (Latent Semantic Analysis)\n",
        "*   Topic model - LDA (Latent Dirichlet Allocation)\n",
        "*   Embeddings - pre-trained\n",
        "*   Embeddings - corpus-trained\n",
        "\n",
        "Distance measures:\n",
        "*   Standard distance measures e.g. cosine etc.\n",
        "*   Non-standard distance measures e.g. Word Movers Distance\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFCqKE4TabnB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "album_corpus = pickle.load( open( \"master_table.pkl\", \"rb\" ) )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZU9l6zvUvAE",
        "colab_type": "text"
      },
      "source": [
        "## Standard distance measures"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SmPnkn8-UL8F",
        "colab": {}
      },
      "source": [
        "from math import*\n",
        "from decimal import Decimal\n",
        " \n",
        "class Similarity():\n",
        "  \n",
        "  def euclidean_distance(self,x,y):\n",
        "          return sqrt(sum(pow(a-b,2) for a, b in zip(x, y)))\n",
        "\n",
        "  def manhattan_distance(self,x,y):\n",
        "          return sum(abs(a-b) for a,b in zip(x,y))\n",
        "\n",
        "  def minkowski_distance(self,x,y,p_value):\n",
        "          return self.nth_root(sum(pow(abs(a-b),p_value) for a,b in zip(x, y)),\n",
        "             p_value)\n",
        "\n",
        "  def cosine_similarity(self,x,y):\n",
        "          numerator = sum(a*b for a,b in zip(x,y))\n",
        "          denominator = self.square_rooted(x)*self.square_rooted(y)\n",
        "          return round(numerator/float(denominator),3)\n",
        "\n",
        "  def jaccard_similarity(self,x,y):\n",
        "          intersection_cardinality = len(set.intersection(*[set(x), set(y)]))\n",
        "          union_cardinality = len(set.union(*[set(x), set(y)]))\n",
        "          return intersection_cardinality/float(union_cardinality)\n",
        "        \n",
        "  def nth_root(self,value, n_root):\n",
        "          root_value = 1/float(n_root)\n",
        "          return round (Decimal(value) ** Decimal(root_value),3)\n",
        "      \n",
        "  def square_rooted(self,x): \n",
        "          return round(sqrt(sum([a*a for a in x])),3)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlyUTKaMrkwx",
        "colab_type": "text"
      },
      "source": [
        "## Vector space features and standard distance measures"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QP2BjYP_Xz9P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import nltk\n",
        "from nltk import word_tokenize          \n",
        "from nltk.stem import WordNetLemmatizer\n",
        "measures = Similarity()\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords', quiet=True, raise_on_error=True)\n",
        "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "tokenized_stop_words = nltk.word_tokenize(' '.join(nltk.corpus.stopwords.words('english')))\n",
        " \n",
        "class LemmaTokenizer(object):\n",
        "  \n",
        "    def __init__(self):\n",
        "        nltk.download('punkt', quiet=True, raise_on_error=True)\n",
        "        self.stemmer = nltk.stem.PorterStemmer()\n",
        "        \n",
        "    def _stem(self, token):\n",
        "        if (token in stop_words):\n",
        "            return token  # Solves error \"UserWarning: Your stop_words may be inconsistent with your preprocessing.\"\n",
        "        return self.stemmer.stem(token)\n",
        "        \n",
        "    def __call__(self, line):\n",
        "        tokens = nltk.word_tokenize(line)\n",
        "        tokens = (self._stem(token) for token in tokens)  # Stemming\n",
        "        return list(tokens)\n",
        "\n",
        "\n",
        "def distance_vectors(corpus, stringlist, vect=TfidfVectorizer, dist=measures.euclidean_distance):\n",
        "  \n",
        "  ###vectorizer\n",
        "  t_vectorizer = vect(tokenizer=LemmaTokenizer(),\n",
        "                      strip_accents='unicode',\n",
        "                      stop_words=tokenized_stop_words,\n",
        "                      lowercase=True,\n",
        "                      ngram_range=(1,4),\n",
        "                      analyzer='word')\n",
        "\n",
        "  X_t = t_vectorizer.fit_transform(corpus)\n",
        "  test_t = t_vectorizer.transform(stringlist)\n",
        "  \n",
        "  ###similarity calculation\n",
        "  scores = []\n",
        "  for i in range(0,len(corpus)):    \n",
        "    scores.append(dist(test_t.toarray()[0],X_t[i].toarray()[0]))\n",
        "\n",
        "  ###print top 3 most similar\n",
        "  indices = np.array(scores).argsort()[0:3]\n",
        "  for i in indices:\n",
        "    values = album_corpus.iloc[i][1:3]\n",
        "    print(values.values)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwwyOn7XGucV",
        "colab_type": "code",
        "outputId": "70dd8e06-86d3-42c3-c246-9746b14fa4f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "###vector space test - working well! returning logical results\n",
        "distance_vectors(corpus=album_corpus['review_text'],stringlist=['a heavy drum section followed by uplifting chorus, with rock and roll influences'])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Darkthrone' 'Old Star']\n",
            "['T. Rex' 'The Slider']\n",
            "['Bob Dylan' 'Bob Dylan: Rolling Thunder Revue: The 1975 Live Recordings']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hl0_oyGerzqO",
        "colab_type": "text"
      },
      "source": [
        "## Embedding features and standard distance measures\n",
        "\n",
        "This will be an average of the individual word embeddings for each sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPSbglcuUzEz",
        "colab_type": "text"
      },
      "source": [
        "## Embedding features and non-standard distance measures\n",
        "\n",
        "WMD is an embedding specific distance measure; it assesses the \"distance\" between two documents in a meaningful way, even when they have no words in common, by using word2vec vector embeddings of words.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jPPYWCt_Ju-",
        "colab_type": "code",
        "outputId": "70ad754c-d2f3-4a72-86e4-a8ace7e3fccc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models import Phrases\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import download\n",
        "download('stopwords')\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "def distance_embeddings(corpus, stringlist,trained=False):\n",
        "  \n",
        "  ###pre-processing\n",
        "  def pre_processor(list):\n",
        "    pp_corpus=[]\n",
        "    for i in list:\n",
        "      i = i.lower().split()\n",
        "      i = [w for w in i if w not in stop_words]\n",
        "      pp_corpus.append(i)\n",
        "    return pp_corpus \n",
        "\n",
        "  pp_corpus = pre_processor(corpus)\n",
        "\n",
        "\n",
        "  ###word embeddings\n",
        "  #bigram_transformer = Phrases(album_corpus['review_text'])\n",
        "  if trained == False:\n",
        "    word_model = Word2Vec(pp_corpus, min_count=2, size=100, window=5, iter=100)\n",
        "  else:\n",
        "    word_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
        "  ###similarity calculation\n",
        "  scores = []\n",
        "  for i in range(0,len(corpus)):    \n",
        "    scores.append(word_model.wmdistance(stringlist[0],pp_corpus[i]))\n",
        "\n",
        "\n",
        "  ###print top 3 most similar\n",
        "  indices = np.array(scores).argsort()[0:3]\n",
        "  for i in indices:\n",
        "    values = album_corpus.iloc[i][1:3]\n",
        "    print(values.values)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCI5lKY4zJBI",
        "colab_type": "code",
        "outputId": "46d7e100-3c65-4736-98e9-3624e1adde2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "###this is not working well...\n",
        "distance_embeddings(corpus=album_corpus['review_text'],stringlist=['a heavy drum section followed by uplifting chorus, with rock and roll influences'])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: DeprecationWarning: Call to deprecated `wmdistance` (Method will be removed in 4.0.0, use self.wv.wmdistance() instead).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['Slowthai' 'Nothing Great About Britain']\n",
            "['Megan Thee Stallion' 'Fever']\n",
            "['Don Cherry' 'Brown Rice']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0N3m8CFxXqej",
        "colab_type": "code",
        "outputId": "9275ed10-1793-40c3-f7ec-82affac7fac1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "###this is not working well...\n",
        "distance_embeddings(corpus=album_corpus['review_text'],stringlist=['a heavy drum section followed by uplifting chorus, with rock and roll influences'],trained=True)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['Káryyn' 'The Quanta Series']\n",
            "['Lil Nas X' '7 EP']\n",
            "['Big K.R.I.T.' 'K.R.I.T. IZ HERE']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkTYoeuRjOs-",
        "colab_type": "text"
      },
      "source": [
        "# Next steps\n",
        "\n",
        "lookup list for UMG artists \\\\\n",
        "alternative websites - any decent music.com \\\\\n",
        "lyric search \\\\\n",
        "artist cluster \\\\"
      ]
    }
  ]
}