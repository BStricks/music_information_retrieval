{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "music_document_match.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "554vObhbHY5e",
        "colab_type": "text"
      },
      "source": [
        "# Section 2: Sentence matching"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cx-ZupnOHUeU",
        "colab_type": "text"
      },
      "source": [
        "## Approach 1 - sentence level matching\n",
        "The first approach taken is to split each review into sentence level data and then perform matching on a 'user -> review sentence' basis. It is hypothesised that given the complex nature of reviews that this will perform better than creating a document level match.\n",
        "\n",
        "Features:\n",
        "*   Vector space model - count vectors (1-4gram)\n",
        "*   Vector space model - tfidf vectors (1-4gram)\n",
        "*   Topic model - LSA (Latent Semantic Analysis)\n",
        "*   Topic model - LDA (Latent Dirichlet Allocation)\n",
        "*   Embeddings - pre-trained\n",
        "*   Embeddings - corpus-trained\n",
        "\n",
        "Distance measures:\n",
        "*   Standard distance measures e.g. cosine etc.\n",
        "*   Non-standard distance measures e.g. Word Movers Distance\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feKUDmzJHhoy",
        "colab_type": "text"
      },
      "source": [
        "## Standard distance measures"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2BfbLvqHRSl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from math import*\n",
        "from decimal import Decimal\n",
        " \n",
        "class Similarity():\n",
        "  \n",
        "  def euclidean_distance(self,x,y):\n",
        "          return sqrt(sum(pow(a-b,2) for a, b in zip(x, y)))\n",
        "\n",
        "  def manhattan_distance(self,x,y):\n",
        "          return sum(abs(a-b) for a,b in zip(x,y))\n",
        "\n",
        "  def minkowski_distance(self,x,y,p_value):\n",
        "          return self.nth_root(sum(pow(abs(a-b),p_value) for a,b in zip(x, y)),\n",
        "             p_value)\n",
        "\n",
        "  def cosine_similarity(self,x,y):\n",
        "          numerator = sum(a*b for a,b in zip(x,y))\n",
        "          denominator = self.square_rooted(x)*self.square_rooted(y)\n",
        "          return round(numerator/float(denominator),3)\n",
        "\n",
        "  def jaccard_similarity(self,x,y):\n",
        "          intersection_cardinality = len(set.intersection(*[set(x), set(y)]))\n",
        "          union_cardinality = len(set.union(*[set(x), set(y)]))\n",
        "          return intersection_cardinality/float(union_cardinality)\n",
        "        \n",
        "  def nth_root(self,value, n_root):\n",
        "          root_value = 1/float(n_root)\n",
        "          return round (Decimal(value) ** Decimal(root_value),3)\n",
        "      \n",
        "  def square_rooted(self,x): \n",
        "          return round(sqrt(sum([a*a for a in x])),3)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfOuUcEHHlqG",
        "colab_type": "text"
      },
      "source": [
        "## Vector space features and standard distance measures"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1P_Cs98hHozw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import nltk\n",
        "from nltk import word_tokenize          \n",
        "from nltk.stem import WordNetLemmatizer\n",
        "measures = Similarity()\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords', quiet=True, raise_on_error=True)\n",
        "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "tokenized_stop_words = nltk.word_tokenize(' '.join(nltk.corpus.stopwords.words('english')))\n",
        " \n",
        "class LemmaTokenizer(object):\n",
        "  \n",
        "    def __init__(self):\n",
        "        nltk.download('punkt', quiet=True, raise_on_error=True)\n",
        "        self.stemmer = nltk.stem.PorterStemmer()\n",
        "        \n",
        "    def _stem(self, token):\n",
        "        if (token in stop_words):\n",
        "            return token  # Solves error \"UserWarning: Your stop_words may be inconsistent with your preprocessing.\"\n",
        "        return self.stemmer.stem(token)\n",
        "        \n",
        "    def __call__(self, line):\n",
        "        tokens = nltk.word_tokenize(line)\n",
        "        tokens = (self._stem(token) for token in tokens)  # Stemming\n",
        "        return list(tokens)\n",
        "\n",
        "\n",
        "def distance_vectors(corpus, stringlist, vect=TfidfVectorizer, dist=measures.euclidean_distance):\n",
        "  \n",
        "  ###vectorizer\n",
        "  t_vectorizer = vect(tokenizer=LemmaTokenizer(),\n",
        "                      strip_accents='unicode',\n",
        "                      stop_words=tokenized_stop_words,\n",
        "                      lowercase=True,\n",
        "                      ngram_range=(1,4),\n",
        "                      analyzer='word')\n",
        "\n",
        "  X_t = t_vectorizer.fit_transform(corpus)\n",
        "  test_t = t_vectorizer.transform(stringlist)\n",
        "  \n",
        "  ###similarity calculation\n",
        "  scores = []\n",
        "  for i in range(0,len(corpus)):    \n",
        "    scores.append(dist(test_t.toarray()[0],X_t[i].toarray()[0]))\n",
        "\n",
        "  ###print top 3 most similar\n",
        "  indices = np.array(scores).argsort()[0:3]\n",
        "  for i in indices:\n",
        "    values = album_corpus.iloc[i][1:3]\n",
        "    print(values.values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbWrYWTvIfQt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "###change directory\n",
        "%cd gdrive/My Drive/Colab Notebooks/album_reviews\n",
        "\n",
        "album_corpus = pickle.load( open( \"album_corpus.pkl\", \"rb\" ) )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KE0aU1YXHttG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###vector space test - working well! returning logical results\n",
        "distance_vectors(corpus=album_corpus['review_text'],stringlist=['a heavy drum section followed by uplifting chorus, with rock and roll influences'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsEh1tO5HwwG",
        "colab_type": "text"
      },
      "source": [
        "## Embedding features and non-standard distance measures\n",
        "\n",
        "WMD is an embedding specific distance measure; it assesses the \"distance\" between two documents in a meaningful way, even when they have no words in common, by using word2vec vector embeddings of words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34iGn0P2Hw-A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models import Phrases\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import download\n",
        "download('stopwords')\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "def distance_embeddings(corpus, stringlist,trained=False):\n",
        "  \n",
        "  ###pre-processing\n",
        "  def pre_processor(list):\n",
        "    pp_corpus=[]\n",
        "    for i in list:\n",
        "      i = i.lower().split()\n",
        "      i = [w for w in i if w not in stop_words]\n",
        "      pp_corpus.append(i)\n",
        "    return pp_corpus \n",
        "\n",
        "  pp_corpus = pre_processor(corpus)\n",
        "\n",
        "\n",
        "  ###word embeddings\n",
        "  #bigram_transformer = Phrases(album_corpus['review_text'])\n",
        "  if trained == False:\n",
        "    word_model = Word2Vec(pp_corpus, min_count=2, size=100, window=5, iter=100)\n",
        "  else:\n",
        "    word_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
        "  ###similarity calculation\n",
        "  scores = []\n",
        "  for i in range(0,len(corpus)):    \n",
        "    scores.append(word_model.wmdistance(stringlist[0],pp_corpus[i]))\n",
        "\n",
        "\n",
        "  ###print top 3 most similar\n",
        "  indices = np.array(scores).argsort()[0:3]\n",
        "  for i in indices:\n",
        "    values = album_corpus.iloc[i][1:3]\n",
        "    print(values.values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2_3pQcsH6q-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###this is not working well...\n",
        "distance_embeddings(corpus=album_corpus['review_text'],stringlist=['a heavy drum section followed by uplifting chorus, with rock and roll influences'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDEgtXUZIAGD",
        "colab_type": "text"
      },
      "source": [
        "# Next steps\n",
        "\n",
        "lookup list for UMG artists \\\\\n",
        "alternative websites - any decent music.com \\\\\n",
        "lyric search \\\\\n",
        "artist cluster \\\\"
      ]
    }
  ]
}